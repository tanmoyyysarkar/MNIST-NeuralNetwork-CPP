# MNIST-NeuralNetwork-CPP

# ğŸ§  Neural Network from Scratch (C++)

This project implements a simple feedforward neural network in **C++** trained on the **MNIST dataset**.
It is built entirely from scratch â€” no external machine learning libraries are used â€” to demonstrate how neural networks work under the hood.

---

## âœ¨ Features
- Implementation of feedforward neural network
- Stochastic Gradient Descent (SGD) with mini-batches
- Backpropagation algorithm
- Sigmoid activation function
- Cost function (Quadratic)
- Trained & tested on the MNIST dataset

---

## ğŸ“‚ Project Structure
.
â”œâ”€â”€ headers/ # Header files (class definitions, function prototypes)
â”œâ”€â”€ src/ # Source files (class implementations)
â”œâ”€â”€ data/ # Dataset (MNIST files, ignored in git)
â”œâ”€â”€ main.cpp # Entry point
â”œâ”€â”€ utils.cpp/.hpp # Utility functions
â”œâ”€â”€ Network.cpp/.hpp # Neural network implementation
â”œâ”€â”€ README.md # Project documentation
â””â”€â”€ .gitignore # Ignore datasets, build files, etc.

yaml
Copy
Edit

---

## âš¡ Getting Started

### 1. Clone the repository
```bash
git clone https://github.com/your-username/your-repo-name.git
cd your-repo-name
2. Build the project
Using g++:

bash
Copy
Edit
g++ main.cpp src/*.cpp -o nn -O2
3. Run
bash
Copy
Edit
./nn
ğŸ“Š Dataset
This project uses the MNIST handwritten digits dataset.
Download from Yann LeCunâ€™s MNIST page and place the files inside the data/ folder.

Files needed:

train-images.idx3-ubyte

train-labels.idx1-ubyte

t10k-images.idx3-ubyte

t10k-labels.idx1-ubyte

âš ï¸ The dataset is not included in the repo (see .gitignore).

ğŸ“– How It Works
Feedforward â€“ Computes activations layer by layer.

Backpropagation â€“ Calculates gradients of cost function wrt weights & biases.

SGD â€“ Updates weights in mini-batches using gradient descent.

Evaluation â€“ Compares predictions with labels for accuracy.

âœ… Results
On MNIST test set (10,000 digits), the network achieves around X% accuracy (after Y epochs).
(Fill this in once you run your model!)

ğŸ› ï¸ Future Improvements
Add ReLU activation

Support cross-entropy loss

Save & load trained models

Multi-threading for faster training

ğŸ¤ Contributing
Pull requests and suggestions are welcome!
If youâ€™d like to contribute, fork the repo and open a PR.

ğŸ“œ License
This project is licensed under the MIT License.
